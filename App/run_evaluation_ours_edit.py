import os
import json
import base64
import pandas as pd
import re
import glob
from tqdm import tqdm
from openai import OpenAI

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
# 1. å¾…è¯„ä¼°çš„ç”Ÿæˆç»“æœæ ¹ç›®å½•
EVAL_ROOT = "/data5/shaochenyang/Workspace/AutoSciFigure2/Eval/Our_Batch_Generation/Batch_Run_01"

# 2. æ•°æ®é›†è·¯å¾„ (ç”¨äºè·å– Prompt ä¸Šä¸‹æ–‡ï¼Œè¾…åŠ© Log)
DATASET_PATH = "/data5/shaochenyang/AI_Scientist/AutoSciFigure/VisualDeepResearch/Construct/output/iclr_2024_figures_dataset.jsonl"

# 3. è¾“å‡ºæ–‡ä»¶å
MODEL_NAME = "LiveFigure_Ours"
OUTPUT_REPORT_PATH = f"/data5/shaochenyang/Workspace/AutoSciFigure2/Eval/Evaluation_Edit/evaluation_report_SED_{MODEL_NAME}.csv"
OUTPUT_SUMMARY_PATH = f"/data5/shaochenyang/Workspace/AutoSciFigure2/Eval/Evaluation_Edit/evaluation_summary_SED_{MODEL_NAME}.md"

# 4. LLM Configuration
JUDGE_MODEL = "gpt-5" # Recommend using the strongest model for SED judgment
API_KEY = os.getenv("API_KEY", "YOUR_API_KEY_HERE")
API_BASE = os.getenv("API_BASE", "YOUR_API_BASE_URL") 

# ==============================================

class SEDEvaluator:
    def __init__(self):
        print(f"ğŸ”§ [Init] åˆå§‹åŒ– SED è¯„ä¼°è£åˆ¤: {JUDGE_MODEL}")
        self.client = OpenAI(api_key=API_KEY, base_url=API_BASE)
        self.results = []
        self.prompt_lookup = self._load_dataset_prompts()

    def _clean_filename(self, text):
        if not text: return "Unknown"
        clean_text = re.sub(r'[\\/*?:"<>|]', '_', text)
        clean_text = clean_text.replace(" ", "_")
        clean_text = re.sub(r'_+', '_', clean_text)
        return clean_text.strip()[:100]

    def _load_dataset_prompts(self):
        print(f"ğŸ“– Loading dataset metadata...")
        lookup = {}
        try:
            with open(DATASET_PATH, 'r', encoding='utf-8') as f:
                for line in f:
                    item = json.loads(line)
                    p_name = self._clean_filename(item.get("paper_name", "Unknown"))
                    f_label = self._clean_filename(item.get("figure_label", "Fig"))
                    case_id = f"{p_name}_{f_label}"
                    lookup[case_id] = {
                        "caption": item.get("caption", ""),
                        "description": item.get("description", "")
                    }
        except Exception as e:
            print(f"âš ï¸ Failed to load dataset: {e}")
        return lookup

    def encode_image(self, image_path):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def _find_best_image(self, folder_path):
        """
        æŸ¥æ‰¾ç­–ç•¥ï¼šiter_2 > iter_1 > iter_0
        """
        if not os.path.exists(folder_path):
            return None
        
        all_files = [f for f in os.listdir(folder_path) if f.endswith(".png")]
        
        for target_iter in ["iter_2", "iter_1", "iter_0"]:
            candidates = [f for f in all_files if target_iter in f]
            # æ’é™¤ assets å’Œ reference
            candidates = [f for f in candidates if "reference" not in f and "assets" not in f]
            
            if candidates:
                candidates.sort() # å­—æ¯åºæ’åºï¼Œå–æœ€åä¸€ä¸ª (try_x æœ€å¤§å€¼)
                best_img = candidates[-1]
                return os.path.join(folder_path, best_img)
        return None

    # =========================================================================
    # æ ¸å¿ƒï¼šè®¡ç®— Semantic Edit Distance (SED)
    # =========================================================================
    def evaluate_sed(self, gt_path, gen_path):
        """
        è¾“å…¥ï¼šGTå›¾ç‰‡è·¯å¾„ï¼Œç”Ÿæˆå›¾ç‰‡è·¯å¾„
        è¾“å‡ºï¼šJSON (Steps, Plan, Analysis)
        """
        if not os.path.exists(gen_path): return None
        
        # å¤„ç† GT å›¾ç‰‡ (å…¼å®¹ jpg/png)
        real_gt_path = gt_path
        if not os.path.exists(real_gt_path):
            real_gt_path = gt_path.replace(".jpg", ".png")
            if not os.path.exists(real_gt_path):
                # å¦‚æœæ‰¾ä¸åˆ° GTï¼Œæ— æ³•è®¡ç®— SED
                return None

        b64_gen = self.encode_image(gen_path)
        b64_gt = self.encode_image(real_gt_path)

        # Prompt: è¿™é‡Œçš„å®šä¹‰éå¸¸å…³é”®ï¼Œç›´æ¥å†³å®šäº† SED çš„ç²’åº¦
        system_prompt = """
        You are a Senior Scientific Editor and Layout Engineer.
        Your task is to evaluate a **Generated Scientific Figure** against a **Ground Truth (Reference)**.
        
        GOAL:
        Determine the **Semantic Edit Distance (SED)**, which is the sequence of Atomic Operations required to transform the Generated Figure into a publication-ready state that matches the information fidelity and visual standard of the Ground Truth.
        
        NOTE:
        - You do NOT need pixel-perfect matching.
        - Focus on Information Accuracy (Text, Topology) and Visual Clarity (Layout, Style).
        - If the generated figure is already perfect or semantically equivalent, return an empty list.

        DEFINITIONS OF ATOMIC OPERATIONS (1 Step each):
        1. [TEXT_EDIT]: Fix typo, change text content, or adjust font size/weight.
        2. [MOVE]: Move a SINGLE object/group to a correct position.
        3. [RESIZE]: Resize a SINGLE object.
        4. [STYLE]: Change color, border, or shape style of a SINGLE object.
        5. [ADD]: Add a missing object or arrow.
        6. [DELETE]: Remove a hallucinated or unnecessary object.
        7. [CONNECT]: Fix or reroute a connection arrow.

        STRICT RULES:
        - Be granular. Do not say "Fix layout" (which is vague). Say "Move Box A", "Move Box B", "Resize Box C".
        - Count steps conservatively but accurately.
        
        OUTPUT FORMAT (JSON ONLY):
        {
            "analysis": "Brief analysis of the main differences...",
            "edit_plan": [
                {"step": 1, "type": "TEXT_EDIT", "description": "Change 'Hellow' to 'Hello' in the blue box"},
                {"step": 2, "type": "MOVE", "description": "Move the 'Encoder' block to the left"},
                ...
            ],
            "total_steps": <int count of list>,
            "is_publication_ready": <bool>
        }
        """

        user_content = [
            {"type": "text", "text": "Reference Ground Truth (Target Standard):"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_gt}"}},
            {"type": "text", "text": "Generated Figure (To be fixed):"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_gen}"}},
            {"type": "text", "text": "Please list the atomic edit operations required."}
        ]

        try:
            response = self.client.chat.completions.create(
                model=JUDGE_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                response_format={"type": "json_object"},
                temperature=0.2 
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"  âš ï¸ API Error during SED calculation: {e}")
            return None

    # =========================================================================
    # ä¸»å¾ªç¯
    # =========================================================================
    def run(self):
        if not os.path.exists(EVAL_ROOT):
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {EVAL_ROOT}")
            return

        cases = sorted([d for d in os.listdir(EVAL_ROOT) if os.path.isdir(os.path.join(EVAL_ROOT, d))])
        print(f"ğŸš€ å¼€å§‹ SED è¯„ä¼° | å…± {len(cases)} ä¸ª Case | æ¨¡å‹: {JUDGE_MODEL}")

        for case_name in tqdm(cases, desc="Evaluating SED"):
            case_dir = os.path.join(EVAL_ROOT, case_name)
            
            # æŸ¥æ‰¾ GT
            gt_path = os.path.join(case_dir, "ground_truth.jpg")
            
            # å®šä¹‰ V1 å’Œ V2 ç›®å½•
            v1_dir = os.path.join(case_dir, "V1_CaptionOnly")
            v2_dir = os.path.join(case_dir, "V2_WithContext")
            
            # æ‰¾åˆ°æœ€ä½³ç”Ÿæˆå›¾
            v1_img = self._find_best_image(v1_dir)
            v2_img = self._find_best_image(v2_dir)

            for ver_label, img_path in [("V1", v1_img), ("V2", v2_img)]:
                if not img_path: continue
                
                # è®¡ç®— SED
                res = self.evaluate_sed(gt_path, img_path)
                
                if res:
                    # å°† edit_plan è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿å­˜å…¥ CSV
                    plan_str = json.dumps(res.get("edit_plan", []), ensure_ascii=False)
                    
                    record = {
                        "CaseID": case_name,
                        "Version": ver_label,
                        "SED_Score": res.get("total_steps", 999), # 999 è¡¨ç¤ºå¼‚å¸¸
                        "Is_Publication_Ready": res.get("is_publication_ready", False),
                        "Analysis": res.get("analysis", ""),
                        "Edit_Plan": plan_str[:3000], # é˜²æ­¢ CSV çˆ†æ‰ï¼Œæˆªæ–­ä¸€ä¸‹
                        "Used_Image": os.path.basename(img_path)
                    }
                    self.results.append(record)
                    
                    # å®æ—¶æ‰“å°æ¯”è¾ƒå¥½çš„ç»“æœï¼ˆå¯é€‰ï¼‰
                    # if res.get("total_steps", 999) == 0:
                    #    tqdm.write(f"ğŸ‰ Perfect Match found: {case_name} ({ver_label})")

        self.save_reports()

    def save_reports(self):
        if not self.results: 
            print("âš ï¸ æ²¡æœ‰äº§ç”Ÿç»“æœ")
            return
            
        df = pd.DataFrame(self.results)
        df.to_csv(OUTPUT_REPORT_PATH, index=False)
        print(f"\nâœ… è¯¦ç»†æ—¥å¿—å·²ä¿å­˜: {OUTPUT_REPORT_PATH}")

        # ç»Ÿè®¡åˆ†æ
        # è®¡ç®— V1 å’Œ V2 çš„å¹³å‡ç¼–è¾‘è·ç¦»
        summary_sed = df.groupby("Version")["SED_Score"].mean().round(2)
        summary_ready = df.groupby("Version")["Is_Publication_Ready"].sum()
        total_count = df.groupby("Version")["CaseID"].count()
        ready_rate = (summary_ready / total_count * 100).round(1)

        print("\n" + "="*60)
        print(f"ğŸ“Š SED Evaluation Summary: {MODEL_NAME}")
        print("="*60)
        print(f"Average SED (Lower is Better):")
        print(summary_sed.to_string())
        print("-" * 60)
        print(f"Publication Ready Rate (Higher is Better):")
        print(ready_rate.to_string())
        print("="*60)

        # ç”Ÿæˆ Markdown æŠ¥å‘Š
        md_content = f"# Semantic Edit Distance (SED) Report\n"
        md_content += f"**Judge Model**: {JUDGE_MODEL}\n\n"
        
        md_content += "## 1. Average SED (Lower is better)\n"
        md_content += summary_sed.to_markdown() + "\n\n"
        
        md_content += "## 2. Publication Ready Rate\n"
        md_content += ready_rate.to_markdown() + "\n\n"
        
        md_content += "## 3. Metric Definition\n"
        md_content += "- **SED**: The number of atomic operations (Text Edit, Move, Resize, Style, Add, Del, Connect) required to fix the image.\n"
        md_content += "- **Atomic Step**: Defined as a single, granular modification action."

        with open(OUTPUT_SUMMARY_PATH, "w") as f:
            f.write(md_content)
        print(f"âœ… Markdown æ€»ç»“å·²ç”Ÿæˆ: {OUTPUT_SUMMARY_PATH}")

if __name__ == "__main__":
    evaluator = SEDEvaluator()
    evaluator.run()