import os
import json
import base64
import pandas as pd
import re
import glob
from tqdm import tqdm
from openai import OpenAI

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
# ä½ ä»¬è‡ªå·±æ–¹æ¡ˆçš„è¾“å‡ºæ ¹ç›®å½•
EVAL_ROOT = "/data5/shaochenyang/Workspace/AutoSciFigure2/Eval/Our_Batch_Generation/Batch_Run_01"

# æ•°æ®é›†è·¯å¾„
DATASET_PATH = "/data5/shaochenyang/AI_Scientist/AutoSciFigure/VisualDeepResearch/Construct/output/iclr_2024_figures_dataset.jsonl"

# æ¨¡å‹åç§°
model_name = "LiveFigure_Ours"

# è¾“å‡ºè·¯å¾„
OUTPUT_REPORT_PATH = f"/data5/shaochenyang/Workspace/AutoSciFigure2/Eval/Evaluation/evaluation_report_9metric_{model_name}.csv"
OUTPUT_SUMMARY_PATH = f"/data5/shaochenyang/Workspace/AutoSciFigure2/Eval/Evaluation/evaluation_summary_9metrics_{model_name}.md"

# ã€å¼€å…³ã€‘æ˜¯å¦å¼€å¯ V2 vs V1 çš„èƒœç‡è¯„ä¼°
ENABLE_PAIRWISE = True

JUDGE_MODEL = "gpt-4o"

class Config:
    API_KEY = os.getenv("API_KEY", "YOUR_API_KEY_HERE")
    API_BASE = os.getenv("API_BASE", "YOUR_API_BASE_URL") 
# ==============================================

class AutoEvaluator:
    def __init__(self):
        print(f"ğŸ”§ [Init] åˆå§‹åŒ–è¯„ä¼°è£åˆ¤: {JUDGE_MODEL} | ç›®æ ‡æ¨¡å‹: {model_name}")
        self.client = OpenAI(
            api_key=Config.API_KEY, 
            base_url=Config.API_BASE
        )
        self.results = []
        self.pairwise_results = {"V1_Wins": 0, "V2_Wins": 0, "Tie": 0, "Total": 0}
        
        self.prompt_lookup = self._load_dataset_prompts()

    def _clean_filename(self, text):
        if not text: return "Unknown"
        clean_text = re.sub(r'[\\/*?:"<>|]', '_', text)
        clean_text = clean_text.replace(" ", "_")
        clean_text = re.sub(r'_+', '_', clean_text)
        return clean_text.strip()[:100]

    def _load_dataset_prompts(self):
        print(f"ğŸ“– Loading prompts from dataset...")
        lookup = {}
        try:
            with open(DATASET_PATH, 'r', encoding='utf-8') as f:
                for line in f:
                    item = json.loads(line)
                    p_name = self._clean_filename(item.get("paper_name", "Unknown"))
                    f_label = self._clean_filename(item.get("figure_label", "Fig"))
                    case_id = f"{p_name}_{f_label}"
                    lookup[case_id] = {
                        "caption": item.get("caption", ""),
                        "description": item.get("description", "")
                    }
        except Exception as e:
            print(f"âš ï¸ Failed to load dataset: {e}")
        return lookup

    def encode_image(self, image_path):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    # =========================================================================
    # æ ¸å¿ƒé€»è¾‘ï¼šæ™ºèƒ½æŸ¥æ‰¾æœ€æ–°çš„ iter å›¾ç‰‡
    # =========================================================================
    def _find_best_image(self, folder_path):
        if not os.path.exists(folder_path):
            return None
        
        # è·å–æ‰€æœ‰ png æ–‡ä»¶
        all_files = [f for f in os.listdir(folder_path) if f.endswith(".png")]
        
        # ä¼˜å…ˆçº§è§„åˆ™: iter_2 > iter_1 > iter_0
        for target_iter in ["iter_2", "iter_1", "iter_0"]:
            # ç­›é€‰å‡ºåŒ…å«å½“å‰ iter å…³é”®å­—çš„æ–‡ä»¶
            candidates = [f for f in all_files if target_iter in f]
            
            # æ’é™¤æ‰ reference å›¾ç‰‡ (å¦‚ 00_reference_gemini.png) å’Œ assets
            candidates = [f for f in candidates if "reference" not in f and "assets" not in f]
            
            if candidates:
                # å¦‚æœæœ‰å¤šä¸ª (ä¾‹å¦‚ try_0, try_1)ï¼ŒæŒ‰å­—æ¯æ’åºå–æœ€åä¸€ä¸ª (é€šå¸¸ä»£è¡¨æœ€æ–°å°è¯•)
                candidates.sort()
                best_img = candidates[-1]
                return os.path.join(folder_path, best_img)
        
        return None

    # =========================================================================
    # Task 1: 9-Metric Scoring (å®Œå…¨ä¸€è‡´çš„ Prompts)
    # =========================================================================
    def evaluate_comprehensive(self, gt_path, gen_path, prompt_text):
        if not os.path.exists(gen_path): return None
        
        base64_gen = self.encode_image(gen_path)
        base64_gt = None
        
        if os.path.exists(gt_path):
            base64_gt = self.encode_image(gt_path)
        else:
            gt_png = gt_path.replace(".jpg", ".png")
            if os.path.exists(gt_png):
                base64_gt = self.encode_image(gt_png)

        # âœ… ã€å®Œå…¨ä¸€è‡´ã€‘Prompt
        system_prompt = """
        You are a Senior Scientific Reviewer. Evaluate the "Generated Scientific Diagram" based on the Input Text and Ground Truth (if provided).
        
        Score the diagram (1-10) across 3 Dimensions and 9 Specific Metrics:

        **Dimension 1: Visual Design Excellence**
        1. Aesthetic Quality: Color harmony, layout modernity, visual appeal.
        2. Visual Expressiveness: Use of meaningful icons/metaphors vs simple boxes.
        3. Professional Polish: Alignment, spacing, vector-quality details.

        **Dimension 2: Communication Effectiveness**
        4. Clarity: Visual hierarchy, ease of understanding, lack of clutter.
        5. Logical Flow: Narrative direction (e.g. left-to-right), clear input-output path.
        6. Text Legibility: Text readability, no gibberish, correct spelling.

        **Dimension 3: Content Fidelity**
        7. Accuracy: Correct topology and relationships vs Ground Truth/Text.
        8. Completeness: No missing key modules/steps mentioned in text.
        9. Appropriateness: Style matches the target audience (scientific paper).

        Return JSON format ONLY:
        {
            "scores": {
                "aesthetic_quality": int,
                "visual_expressiveness": int,
                "professional_polish": int,
                "clarity": int,
                "logical_flow": int,
                "text_legibility": int,
                "accuracy": int,
                "completeness": int,
                "appropriateness": int
            },
            "reasoning": "string"
        }
        """

        # âœ… ã€å®Œå…¨ä¸€è‡´ã€‘User Content
        user_content = [
            {"type": "text", "text": f"Input Prompt Context:\n{prompt_text[:1000]}"},
            {"type": "text", "text": "Generated Diagram (Target):"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_gen}"}}
        ]
        
        if base64_gt:
            user_content.insert(1, {"type": "text", "text": "Ground Truth (Reference):"})
            user_content.insert(2, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_gt}"}})

        try:
            response = self.client.chat.completions.create(
                model=JUDGE_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                response_format={"type": "json_object"},
                temperature=0.2
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"  âš ï¸ Scoring Error: {e}")
            return None

    # =========================================================================
    # Task 2: Pairwise Win-Rate (å®Œå…¨ä¸€è‡´çš„ Prompts)
    # =========================================================================
    def evaluate_pairwise(self, gt_path, v1_path, v2_path, prompt_text):
        if not os.path.exists(v1_path) or not os.path.exists(v2_path): return None
        
        b64_v1 = self.encode_image(v1_path)
        b64_v2 = self.encode_image(v2_path)
        
        b64_gt = None
        if os.path.exists(gt_path):
            b64_gt = self.encode_image(gt_path)
        else:
            gt_png = gt_path.replace(".jpg", ".png")
            if os.path.exists(gt_png):
                b64_gt = self.encode_image(gt_png)

        system_prompt = """
        You are an expert Art Director. Compare Image A (V1) and Image B (V2) against the Ground Truth.
        Which image is better for a scientific paper?
        Return JSON: {"winner": "A" or "B" or "Tie", "reason": "short explanation"}
        """
        
        user_content = [
            {"type": "text", "text": f"Context:\n{prompt_text[:500]}"},
            {"type": "text", "text": "Image A (Option 1):"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_v1}"}},
            {"type": "text", "text": "Image B (Option 2):"},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_v2}"}}
        ]
        
        if b64_gt:
            user_content.insert(1, {"type": "text", "text": "Ground Truth (Reference):"})
            user_content.insert(2, {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_gt}"}})

        try:
            response = self.client.chat.completions.create(
                model=JUDGE_MODEL,
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_content}],
                response_format={"type": "json_object"}, temperature=0.1
            )
            result = json.loads(response.choices[0].message.content)
            return result.get("winner", "Tie")
        except: return "Tie"

    # =========================================================================
    # Main Execution
    # =========================================================================
    def run(self):
        if not os.path.exists(EVAL_ROOT):
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {EVAL_ROOT}")
            return

        cases = sorted([d for d in os.listdir(EVAL_ROOT) if os.path.isdir(os.path.join(EVAL_ROOT, d))])
        print(f"ğŸš€ å¼€å§‹è¯„ä¼° Ours æ–¹æ¡ˆ | å…± {len(cases)} ä¸ª Case")

        for case_name in tqdm(cases, desc="Judging"):
            case_dir = os.path.join(EVAL_ROOT, case_name)
            
            # Ground Truth é€šå¸¸åœ¨ Case æ ¹ç›®å½•ä¸‹ (æˆ–è€…åœ¨ Dataset ç»“æ„é‡Œ)
            # ä½ çš„ç›®å½•ç»“æ„é‡Œï¼ŒGT å¦‚æœä¸åœ¨æ ¹ç›®å½•ï¼Œå¯èƒ½éœ€è¦é¢å¤–æŒ‡å®š
            # è¿™é‡Œå‡è®¾å®ƒå’Œä¹‹å‰ Baseline çš„é€»è¾‘ä¸€è‡´ï¼ŒGT åœ¨ case_dir ä¸‹
            # æˆ–è€…æˆ‘ä»¬å¯ä»¥å¤ç”¨ V1 ä¸‹é¢çš„ `00_reference_gemini.png` ä½œä¸ºä¼ª GTï¼Ÿ
            # æŒ‰ç…§ä½ ä¹‹å‰çš„ Baseline ä»£ç ï¼ŒGT æ˜¯ `case_dir/ground_truth.jpg`ã€‚
            # ä½ çš„æ–°ç›®å½•é‡Œä¼¼ä¹æ²¡æœ‰ç›´æ¥æ”¾ GTï¼Œå¦‚æœéœ€è¦ä¸¥æ ¼è¯„ä¼°ï¼Œè¯·ç¡®ä¿ GT æ–‡ä»¶å­˜åœ¨ã€‚
            # è¿™é‡Œæˆ‘ä¿æŒåŸé€»è¾‘ï¼Œå°è¯•åœ¨ case_dir ä¸‹æ‰¾ GTã€‚
            gt_path = os.path.join(case_dir, "ground_truth.jpg")
            if not os.path.exists(gt_path): 
                gt_path = os.path.join(case_dir, "ground_truth.png")
            
            # è·å– Prompt
            case_data = self.prompt_lookup.get(case_name, {})
            raw_caption = case_data.get("caption", "")
            raw_desc = case_data.get("description", "")

            # 1. ç‹¬ç«‹æ‰“åˆ†
            # è¿™é‡Œçš„è·¯å¾„æŸ¥æ‰¾é€»è¾‘å˜äº†ï¼Œä½¿ç”¨äº† _find_best_image
            v1_dir = os.path.join(case_dir, "V1_CaptionOnly")
            v2_dir = os.path.join(case_dir, "V2_WithContext")
            
            v1_img_path = self._find_best_image(v1_dir)
            v2_img_path = self._find_best_image(v2_dir)

            for ver, img_path, label in [(v1_dir, v1_img_path, "V1"), (v2_dir, v2_img_path, "V2")]:
                if not img_path: 
                    # print(f"âš ï¸ {case_name} {label} æ²¡æ‰¾åˆ°æœ‰æ•ˆå›¾ç‰‡ï¼Œè·³è¿‡")
                    continue
                
                # æ„é€  Prompt (å®Œå…¨ä¸€è‡´)
                prompt = raw_caption if label == "V1" else f"Caption: {raw_caption}\nContext: {raw_desc[:1000]}"

                res = self.evaluate_comprehensive(gt_path, img_path, prompt)
                
                if res and "scores" in res:
                    scores = res["scores"]
                    record = {
                        "CaseID": case_name, "Version": label,
                        **scores, 
                        "Reasoning": res.get("reasoning", "")[:100],
                        "UsedImage": os.path.basename(img_path) # è®°å½•ç”¨äº†å“ªå¼ å›¾ (iter_2 è¿˜æ˜¯ iter_1)
                    }
                    self.results.append(record)

            # 2. èƒœç‡å¯¹æ¯”
            if ENABLE_PAIRWISE:
                if v1_img_path and v2_img_path:
                    winner = self.evaluate_pairwise(gt_path, v1_img_path, v2_img_path, raw_caption)
                    if winner:
                        self.pairwise_results["Total"] += 1
                        if winner == "A": self.pairwise_results["V1_Wins"] += 1
                        elif winner == "B": self.pairwise_results["V2_Wins"] += 1
                        else: self.pairwise_results["Tie"] += 1

        self.save_reports()

    def save_reports(self):
        if not self.results: return
        df = pd.DataFrame(self.results)
        df.to_csv(OUTPUT_REPORT_PATH, index=False)
        print(f"\nâœ… è¯¦ç»†æ•°æ®å·²ä¿å­˜: {OUTPUT_REPORT_PATH}")

        # èšåˆå¹³å‡åˆ†
        metrics = [
            "aesthetic_quality", "visual_expressiveness", "professional_polish",
            "clarity", "logical_flow", "text_legibility",
            "accuracy", "completeness", "appropriateness"
        ]
        
        valid_metrics = [m for m in metrics if m in df.columns]
        summary = df.groupby("Version")[valid_metrics].mean().round(2)

        print("\n" + "="*60)
        print(f"ğŸ“Š Evaluation Summary: {model_name}")
        print("="*60)
        print(summary.to_string())

        # ç”Ÿæˆ Markdown
        md_content = f"# Evaluation Report: {model_name}\n\n## 1. Metrics Score\n{summary.to_markdown()}"
        
        if ENABLE_PAIRWISE and self.pairwise_results["Total"] > 0:
            total = self.pairwise_results["Total"]
            v1_rate = (self.pairwise_results["V1_Wins"] / total) * 100
            v2_rate = (self.pairwise_results["V2_Wins"] / total) * 100
            tie_rate = (self.pairwise_results["Tie"] / total) * 100
            
            print("\n" + "="*60)
            print(f"ğŸ† Pairwise Win-Rate (V2 Context vs V1 Caption)")
            print(f"   - V1 Wins: {v1_rate:.1f}%")
            print(f"   - V2 Wins: {v2_rate:.1f}%")
            print(f"   - Tie:     {tie_rate:.1f}%")
            print("="*60)
            
            md_content += f"\n\n## 2. Pairwise Win-Rate\n- **V1 Wins**: {v1_rate:.1f}%\n- **V2 Wins**: {v2_rate:.1f}%\n- **Tie**: {tie_rate:.1f}%"

        with open(OUTPUT_SUMMARY_PATH, "w") as f:
            f.write(md_content)
        print(f"âœ… Markdown æŠ¥å‘Šå·²ç”Ÿæˆ: {OUTPUT_SUMMARY_PATH}")

if __name__ == "__main__":
    evaluator = AutoEvaluator()
    evaluator.run()

